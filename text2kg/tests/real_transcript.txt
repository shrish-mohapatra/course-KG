All right. Now it is time to put everything that we've done so far together and talk about how we can actually search for things. So in our last. Lecture. We discussed the creation of an. Inverted. Index. So for each term we. Have a list of documents that contain that term. And that gives us an efficient way of finding. Documents related to. A specific term. Now we want to. Perform. Search on those documents. Ultimately given some. Words in a query that a user's. Typed in. We want a ranked set of documents. That are most relevant. For that query. So we'll consider. Two information retrieval. Models. Again, information retrieval is all about. Finding relevant resources. To fulfill. An information need or in other words, providing. A. Search service. And there are many approaches to information retrieval. If you're interested, the. Wikipedia page for. Information. Retrieval has lots of links to specific models. We'll cover these two. Boolean model and vector space model. Because they are both very popular. And they're both quite efficient computationally, which. Is important for. Scalability. Concerns. So the first. Model we'll talk about is the, Boolean model. That was kind of the main option until mid to late 90s, I guess basically when Google came around. Around that time was when we started introducing additional models. But Boolean models. Support queries in the form of. Boolean expressions. So they use. And or not operations. Within that query. So we could say Apple and Peach saying give me documents containing both apple and peach. So these were actually kind of difficult to use. They were often used by. Experts because experts knew. Exactly what to search for. They knew how to formulate those Boolean queries. As fourth year computer science. Students, you know, formulating a Boolean. Query. Probably isn't that complex to you. But to just the regular person who doesn't. Know what a boolean value is, it's going to be much more challenging. So as the. World. Wide Web kind of evolved and. Became more and more prominent. And it. Grew and grew and grew. We had a. Motivation for other models. Because we wanted to, allow people who didn't know about boolean, queries to. Be able to search for things. On the web as well. So the vector space model we'll talk about. Later in this recording is a perfect. Example of what stemmed from that need. So Boolean model, it views each document as a set of words. So this can be applied along with. Our inverted indexed approach. The query results are exactly precise. For the Boolean model because the document. Either meets the query or it does. Not. Again it's a boolean. Query. It's a binary. Output. It either matches or it does not match. So we have exact precision, but we don't again, have a way of sorting things, which is a downside to the Boolean model. So if we had this query Apple and Peach, we can look up the documents for apples and we can look up the documents for peaches, or apple or peach. I suppose we would stem. Those words, get rid of the. S, and we would look up the documents for apple and peach. So the assumption is that the documents are. Ordered, like. We talked about when we were talking about, our. Index procedure. Because that allows us. To perform a merge of those. Documents. Similar to merge sort. If you are familiar with the. Algorithm. Again, if you're in fourth year computer science, you probably. Have. I hope you've heard of merge sort before. For all queries. It's just a union of all of them. For and queries. It's an intersection or a merger of them. For more. Complex Boolean queries. We're still just coming up with true and false for each document that we might have. There just might be multiple steps involved. In resolving. That particular query. So how can we do it? Well, if we have apple and peach, we can basically look, since it's a. Sorted list. At the documents and say, okay, well, one does not. Occur. In both of these. Or the words don't occur in. Document number one. So we'll go. Forward in. The peach list of documents that we get to document number two. We now have two and two. That's good. Document two goes into. Our result. Because it has both apple and peach inside of it. And we can just continue that process. So we keep moving through. three and four. No. Four and five. No. Five and eight. No. Eight and eight. Yes. So eight goes into our result and so on and so on and so on and so on until we go through that entire list. So for more complex queries. Again, there's just multiple steps. To that procedure, but we're still answering the. Boolean value, or the Boolean. Query. Or calculating that Boolean expression as far as does document X contain this or not contain this. Or and you know, all the different Boolean things we can mix in there. So the main advantage of the Boolean model is that it's very efficient and easy to compute. We're just simplifying a Boolean expression for each of those documents and deciding is it true, or is it false. So the runtime complexity of that, if we're talking just. The two words. Is o of x plus y, where x is how many documents to app. Apple occurred in. Y is how many documents. Occurred in. Because we have to go through each of. Those lists of documents. And merging. If we perform further processing to. Have only. Keywords for. Documents, then it's going to be an even smaller list. We might condense our documents down so we don't index every. Single word we may. Be. Say these are the key words or the common words. Again, either using just frequency data. These are the most common five. Words for. Each document, or using some sort of. Natural language processing. To extract some actual keywords. To describe the overall. Theme of that particular document. So while it is easy and efficient to. Compute the results for that. Boolean kind of model. There are some really key drawbacks. So one of those drawbacks is that it often gives either too many or too few results. So in or query is often going to give you too many results. Because, you know, there's a lot of things that mention Apple. There's a lot of things that are mentioned. Peaches. There's not necessarily a lot of things that mention both, but you're going to get anything that mentioned. Either of them within your query. With an end query, you tend to get two fewer results. So you might end up. With zero results if. You had, you know, X and y and Z, maybe x and y and y and z and x and z appear in many documents, but x, y and z don't. So that that becomes a problem as well. So again, these were generally used by experts who had an idea of exactly how. They could formulate this. Query to get the results that they want. For. A regular user, you. Know, it's just too. Difficult to process. Thousands. Of results. On top of that, there's also the ranking problem. So Boolean is either a match. Or it's. Not. It's a binary measure. So we can't. Rank. Based solely on this boolean information. So there are some. Modifications to this that look for the distance between terms in a document. If you're interested in that. Look up. term proximity operator. So for example, if a word occurs. Within. Six words of each other or. Within the same sentence, then we can get an idea again of how we can rank these documents or how we can. Refine. This boolean. Search model. But it's going to. Require more information. It's going to require more processing. And the solution that we're going to talk about next, the vector space model is kind. Of a better way of approaching it. That can still allow us to combine this idea of boolean things as well. So we need a way to give documents a. Score representing their quality or so. Sorry. Yeah. For example, we wish to give more weight, to a document if it contains. The word several times. Perhaps. So that's going to allow us. To perform ranked retrieval. When I say ranked retrieval, I. Mean we can rank the documents that have matched that. Particular query to say here are the top ten or here are the. Top 11 to 20. Or 21 to 30, etc., etc., etc.. So the vector space model is going to let. Us perform ranked retrieval. We're going to be able to generate a search. Score for a document, and we. Can then search those particular or sort those particular scores. So that we can say one document is better fit. Than the other for this. Particular query. Before we talk about the vector space model, though, we will discuss some general. Terms and measurements. That we will need to use within that vector space model. So what we're going to get with. Ranked. Retrieval is a way of. Using free text queries. So that's just where the user types in what they're looking for. It's more. Accessible to. 'Non-experts. They don't need to know any particular query language. They type in. The words that describe. Exactly what they're kind of looking for. And our goal is to. Rank all the documents by the quality of the match, and. Then we can return the top. X of those particular ones. In order to do that. We need a scoring mechanism. So how can we compute a. Score for. Documents based on, the text that is contained inside of those documents relative to the text that is contained inside what. The user has asked for in their query. So we need to compute a. Score between query terms. And documents. So the first term we'll talk about is the term frequency. So term frequency. Of a term t in a document D is the. Number of times that term t. Occurs in document t. So if we use only the term frequency we might not be happy with the results. So some words occur. Often. Across. Many documents. And I'll have to update the example. I always use for this. Because. Nick Rowe used to. Be an economics. Professor at Carleton. And he has since. Retired. But if you took a, if you took a class. With Nick Rowe. Specifically the. Intro econ. Class, you would have heard him talk about bananas a lot, because every example had to do with bananas. So bananas. Isn't a very useful search term. Because bananas happens everywhere. It doesn't give us a way to. Prioritize different. Lectures. If we had the the. Lecture transcripts from Nick Rhodes economic. Lectures, to use my own, I listened to myself. For hours and hours and hours. A week. And I know that I. Always say words like. Ultimately and essentially and basically and so and on and on and on and on. So those aren't helpful either. We talked about. Stopwords in. The index. But there are other words. That might just happen a lot, in general, not not just. Stopwords, but more meaningful words that happen across a large portion. Of documents. So they aren't helpful from a. Search perspective for. That particular reason. These terms have very little discriminating power. So just using. Term. Frequency, just using how many times something happen in a document might not be the best choice. This was also a problem way back in the day. I'm getting old enough now that I can say that, back in the day, people would just spam. Words. On their pages, or they might even hide those words on the pages and just, you know, whatever people are googling. Let's say people love tacos because people always love tacos. Maybe they just put tacos, tacos, tacos, tacos, tacos, topless taquitos on all. Of their pages over and over and over and. Over again in hidden divs. Or wherever in. Meta data. So you don't actually see it on the page. But a web. Crawler. Is going to find that data and it's going to say, oh, this page is all about tacos, and it's going to then direct it to that particular page. Again, this, this is things people do to move themselves up. Search results. So some terms aren't really good. At distinguishing between certain pages. And it's easy to just kind of spam those terms to make something seem like it is about that thing. On the other hand. Rare terms. Within a document help to distinguish that document. From others. So to use. Lecture transcripts from this course, for example. RESTful design that hasn't been mentioned, I don't think, since week two. So if you search for RESTful. Design that might. Be able to direct you to the lectures from week. Two. Cosine similarity hasn't been mentioned in any lecture yet until this lecture, I don't think. But we'll talk about it in this lecture. So that's a way to say. These words are more rare. So they're actually going to be more useful to us. From a search perspective. So how can we account. For the general prevalence of a term? Term frequency allows us to see how. How much this word occurs in a. Document. But the inverse document. Frequency lets us. Measure the. Rareness of the word so the. Inverse document. Frequency of a term t So this one is not by document. Term frequency. Was. We had term frequency of term t. In document D. The inverse document frequency. Is just a measure of that term as a whole. And it is the log of the total number of documents divided by the number of documents that contain that particular term. So in mostly plain English, it's the. Log of proportion of documents the term appears. In. So the log isn't strictly necessary. We could do. Just a proportion, but. The log is often used to smooth that particular portion. So the difference between. 1% of the documents and 3% of the documents is. Smaller than if it were just. A, linear function. So how do we. Interpret. This IDF value? It gives us. A way to measure how discriminatory a term is. So frequent terms are less informative than rare terms. Like I was saying a minute ago, they also have. A lower IDF. Value. So if a term appears. In all documents, we end up getting n. Over. N. That gives us log of one, which gives us an ID of zero. The more rare a term is, the higher its IDF. Is. So that proportion goes. Up. If it only appears in one. Document, then it's a log of an over one, which is log of n, and that's going to be the highest IDF that we can have. So it's a good term for narrowing search results. To go back. To my micro. Example, you know, so much. Is probably. Mentioned employment in an economics course. All the lectures mentioned banana. Some talk about employment. So it occurs in. Less lectures than banana which is going to allow us to use. That word. To select pages in a better way, to. Use our own course, crawler PageRank. Those are things that are. Talked about more. Specifically in a subset of the lectures. Then again. Just words I happen. To say all the time, in all of my different lectures. So it's going to allow us to find relevant lectures. Better if that. Word has a higher IDF. Value. So over the past couple of slides, we've seen, several different measurements. So the document frequency of T, which is the total number of documents that contain the term t. Term frequency of. A document T. Or a term. T in document D is the. Total number of times t occurs in D. And. The inverse document. Frequency of T, the IDF. Of that particular term. So we'll see. We can actually combine these into one specific measure. And our vector space model for. Queries. Is going to use make use of. That measure. And that measure is called. Tf IDF measurement. And it's one of the the most. Important measures in information retrieval. So the tf IDF value. For a term. T in a document D. Is the product of its term frequency. Weight in that document, and its IDF weight. Across. All documents. Or in other words, it's. A combination of how many a times it. Occurs in. This document. Or another way of interpreting that. Is how much of this document is about this. Particular word. And. How rare this word is in. General. So what gives us a high tf IDF. Waiting for a term in a. Document, increasing occurrences within the document and increasing rarity. So rare terms that appear many. Times in a document. Are favored. And intuitively, this makes a lot of sense if I talk about. Vector. Space model a lot today, and I don't talk about vector space model at all anywhere else, then it makes sense. That this lecture would have a high or the term vector. 'Space model would have a high Tf-Idf weight for this particular lecture. So our. Lecture, if we were trying to represent. It, which we will in a little bit as a. Vector. Might have. A high entry for that particular term or really its three terms vector space. Model, but I think you probably get the idea. So now we'll see how we can actually use this measurement in the vector space. Model or VSM. So in the vector space model, we view each document as a vector. So each of those document vectors has one component. For each term in our dictionary. So for each unique term we have. We have an entry. In a vector that. 'Represents each of those documents. the entry for each of those components is the Tf-Idf weight for. That term. In the document. So if. The term doesn't occur in the document. 'The value is zero. Otherwise it's whatever the Tf-Idf weight. Of that term in that. Document is. So that. Means for every. Document we have, we have a v dimensional vector where v is the size of our. Dictionary, or in other words, the number of unique terms that we. Found. So V is still very large. In this case there are a large number of unique terms. But we're going. To see we actually get to eliminate a lot of this because. The zero terms are going to be. Ignored. So if the word doesn't. Occur, we're. Not going to be interested in it. It won't actually affect the calculations. So we end up being able to still calculate things. Across. This v sized. Vector. In a fairly efficient manner. So document in this case can apply. To the resources we want to search. So let's say. Lecture transcripts. Or page contents. Or whatever else we've crawled. And indexed. but document also. Applies to the search query. So the user types in their search query, we transform that. And treat it as a document as well. It's certainly a very short document. Search queries are often yo five words at. Most, maybe. Ten words. But we still treat them as documents. So the user's query. Like I was saying, is a document. Generally much shorter than our other documents which represent pages or whatever resources we've crawled. We're only going to consider terms. That exist in. That query vector. So that's where we. Get a lot of our, efficiency from. We don't need to calculate something. Over every single word that's in our dictionary or every unique word that we've. Ever found. We're only calculating things. Over this. Q dimensional vector, where q is the number of. Search. Terms that we have. So we have this q dimensional, search document. We also have basically two dimensional documents. In our B2B database for whatever those resources are that we. Want to. Search. Again, we're just ignoring all the. Terms that. Don't actually exist in this query. So we have two dimensional. Vectors for both the query. And for all of our documents that we want to search. So if we can compute similarity between the query document and the database document, then we'll have. A measure of relevance for documents. 'To that search query. So if we add the Tf-Idf values. For the query into the query document. 'We have the Tf-Idf values for each of the documents in our database. we want to measure. The similarity. Between those vectors. If we can do that, then. We have a measure of relevance for documents to that search query. And we can then. Rank documents based on exactly that. Measure. We can return. The top k of those documents. So in order to measure the. Similarity. We can compare the angle between. The search query document. And any. Other. Document in our database. So this is a two dimensional example, of. Where rich is. On the x axis, y. Is on or por is on. 'The y axis. And then we have three documents d1, d2, d3 plotted there. So these values would be the Tf-Idf values. Of rich and poor in. Document one. It would be what is it about. 0.3 rich. And 0.97 poor. So that was like me ten years ago. Very very poor. Not really. Rich. At least in the money sense, we could say I was rich in the fact that my back didn't hurt nearly as much as it does all the time. Now, D2 is about zero point. Eight. Rich. And .65 poor. D3 is almost all. About being rich and. Not really anything about being poor. So those are our three documents. We. Also have our query vectors. 'So the Tf-Idf. Values for the query vector are some. Sort of a mix. It looks like it's like a 45 degree angle. So it's kind of a mix between rich and poor. So if we looked at the angles between all of these vectors, the smaller angle means it's a better match. So from that. We can see D2 is. The most related. It has the smallest angle between it and the query. So this is a 2D example. In general though, we can calculate. This for larger. 'Spaces as well. It's just multi-dimensional vector. We'll do the computation. Over all. Of those dimensions. However the minute they are. And then we can measure the angle between them. Sort from lowest to. Highest angle. And return the. Top k. So the question is how can we actually compute this angle. You know, we can look at. It here and say sure. D2 is closest to Q but if we have millions of documents and we have 15 different dimensions, that's I always got lost in math when we went. Beyond three dimensions. Because I can't see that. We need a way to actually compute that particular angle. And what we're going to. Do is rather than actually measure the. Angle itself, we're going to compute the cosine of the angle and the assumption or the knowledge that we can make use of is that the cosine. Of zero degrees is. 'Equal to one, and the cosine of 90 degrees equals zero. So if we take the, the Tf-Idf values in vectors, they're all going to be positive values. So TF is greater than equal. To zero. IDF is greater. Than equal to zero. Tfidf is those numbers multiplied together that's going to be greater than equal to zero as well. Since all the values are positive in that vector. The largest. Difference we can. Have in a 2D example at least is 90 degrees. In the more general. Example, they're. Just whatever the word is perpendicular, but for. Multiple spaces. Again, I'm not good with math. I don't remember. The term, but I know there is a term for it. It's that, so. Rather. Than. Measuring the angle, we're going to measure the. Cosine or one to calculate the cosine of the angle between these. Two vectors. We're going to. End up then. With a measure in the range 0. To 1. So rather. Than sort by. Lowest angle, the highest angle. We're going to. Instead. Sort from highest. Cosine of angle to lowest cosine of angle. Again, because if it is an angle. Of zero it. Has a cosine of one. If it has an angle of 90 it is a cosine of zero. So in order to compute. These angles, we're going to use the. Measurement called cosine. Similarity, which is another. Fundamental equation within the domain of. Information retrieval. And if you are triggered by math, this is probably. An intimidating looking equation because it has various symbols and summations and. Square root. Signs. And that used. To always freak me out when I was younger. But it's actually. Quite straightforward to compute. So V in this case is. Our query vector. It's the vector of unique words that are in that query. So all of these summations. Are just iterating over each unique term in the. Query. And. Again we're only computing over the terms. In. The query. We're ignoring all other terms. Which gives us a huge efficiency bonus. You know, while we might. Have 500,000 unique terms in our database somewhere. Our query is only going to have 5 or 6 terms a lot of the times. So we're doing very short amount of computation. So the numerator we're just saying for each. index for each of those unique. Words we're going to. Multiply. The vector entry in the query document. Times the Vector entry. In the this particular document's vector. So again the query is a vector. Each document. Is vector. 'They have Tf-Idf values in them in the numerator. We're just going to say for each of. Those indices in that. Query vector we're. Going to multiply the tfidf of the. Query vector times. The document vector. So we do this for one document. we then do the denominator. This is just scaling the vectors. So it's taking the Euclidean norm. it's also very straightforward to compute. Again just multiplying values. That occur in that vector. That gives us the cosine. Similarity between this query. And this one document. So like with the Boolean model this. Value's relatively. Quick to. Compute. We're repeating over. V entries whatever the size of. The query is. And we're talking just the unique. Words in the query as well. They say Apple, apple apple Apple just one. It happened a lot of times, but that's fine. The term frequency. Of Apple would be a very high number because it's makes. Up 100%. Of that particular document. Once we have the cosine. Similarity for all documents, we repeat this process for all. Documents. We can then rank them. And have results. We can add the Boolean model in within this as well. So if we want to combine the idea we. Want to support Boolean queries with. This vector space model. So we can still do ranking. We can use the Boolean model to basically approve documents. Do they match this boolean query if they do. Okay. Now create cosine similarity measures for them and then actually rank them. These are the documents that match the query. This is the ranking of those particular documents. So we'll see a module in a little bit called Elastic. Lunar that we're. Going to allow us to, perform a lot of these computations on our own. And that's going to have something to think. About as far as. Scalability goes. Because we're going to. Be loading a bunch of. Things into Ram. We're going to be processing. It on one computer. And hopefully in the back of your head you're thinking, that's not really all that scalable, and you're completely correct. It's not that scalable, but how we can scale it is to. Distribute that process. So the big part of assignment one, the. Most interesting part, I. Think, is that we can all. Implement our own crops. And we can all. Crawl our own separate. Sections of the internet, and we can all implement our. Own search engines that work. Using one single. Computer. But we can have a. Distributed search server that's. Going to be. Aware of all of our. Individual. Search. Engines. And when a search query comes. In, that individual distributed. Search server is going to ask every search engine. That it knows about. What are the results? They're going to send their results back. We're going to merge those results. And send. A response. To the user. So that's how we can actually scale this up. Even just using single, computers. Even just crawling. Little bits of the internet and doing some of these calculations, we can distribute it. Very. Easily by just having a bunch. Of different search services that are going to be merged through. Some. Coordinated. Middle point that's going to act as the actual. Distributed search service. That the client interacts with. So there's going to be layers, I guess you would say, within that system. And that's. Again is a very interesting. Thing because that's an easy thing to scale up. Larger and larger and larger, just by adding more simple computers to. It. That can do some of. This crawling. And some of this search calculation. So that is. How we can perform search using two different models. So given an inverted. Index we can execute. Search. Queries. We can use the Boolean model or the vector space model. We can also combine them again using Boolean model to kind of filter the documents and then using the vector space model to rank them. And we can actually execute those. Search queries in fairly. Efficient ways as well. So the Boolean model is very simple, but it has some drawbacks. Again, neither are too many or too few results in a. Lot of cases. No real way to rank things. The vector space model can give us ranked retrieval using just plain text search. It doesn't. Require. Significant amounts of additional space or computation over the Boolean model. It just uses some basic values to compute, you know. The term. Frequencies and the IDF values. And we can actually precompute all of those because they don't change. Except for when we read crawl pages. And have to update the actual, frequency data. But as far as our search is concerned. It can just use whatever data we have stored. The IDF values. Can already be computed for every word in our dictionary. That we know about. So now we'll go take a look at, basic use of Elastic Lunar, which is the package I'm going to use in node to, to do the indexing and. To perform the search queries. There's probably similar. I mean, Elastic Lunar I think is available in a bunch of different, languages. But I could be wrong, but they're probably something similar. If not, you can just implement those calculations. They're not that hard. Like I. Said earlier. It's part of my first. Year class is to implement these exact calculations. So if you were in that class and did this project, then the assignment should be definitely easy for you because you've done it before. I'll be. Yeah. Let's go look at Elastic Lunar and. Then we will wrap up for this week. Right. So this is. A really basic example of how to use Elastic. Load. Or there are going to be. Additional things you. Have to figure out as far as making it work with your database, because you're going to have a. Slightly more scalable solution than I. Have in this particular. Example. And how to kind of. Get the references. Back from the database, ultimately, how to multiply. By PageRank in. Order to boost by. PageRank, which is another part of the. Assignment. But this will only show the basics of elastic loops. So here I have three documents. And these documents. Really are what you're going to have stored in your Mongo database or whatever database. You're using. After crawling. So you'll be able to read. Those documents into. Your, program. And then index them in the same kind of way that I'm going to. Do. So I have three documents. Title for document, one Apple, Peach party. And the body for that is Apple, Apple peach peach apple peach apple. Because it's about an. Apple beach party. And that's what you're going to talk about. Obviously, if you're having an apple peach party. document two same idea title and body. So this one is all about apples. This one is a mix about apples and peaches. This one is about bananas and peaches. Those are our three documents. And we'll be able to see. In the search score results. The results we have make sense because we know what these documents are about. Ones about apples and peaches. One's about apples. One's about bananas and peaches. So then with Elastic Luna, you can set up your index. So you can set up your index by specifying what fields you want to have included in your search. You can also specify ways to. Configure it so certain fields are weighted. More. Maybe you want to weight the title more than the body. Maybe you want to weight the body more than the title. Maybe you have link texts and you want to wait. That more, or header texts and you want to wait that more. All of these things are things you. Can incorporate into it. If you want to. You also specify. The reference that you. Want back for a page. So if you have a unique ID for a page, you can say, give me this ID for that page back. And then I can look it up and, you know, add it to a result or something like that. Once you do that, you need to add all your documents into the index. So I have three documents. I can just write three lines of code. If you have a. Giant array of. Documents, you. Can make a for loop. Either way, you have to add them all into the index. That performs those. Steps that we talked about when we were talking indexing as far as stemming and, tokenizing and all of these things, creating that inverted index. It's handled automatically for you by elastic learner that gives you an index that you can then. Run search queries against. Then we're going to run some query. So here's our list of queries. Our first one will. Be just apple apple peach apple apple peaches. So these three should give us the same results. Because the term frequency. Of apple and peach are the. Same. The third. Case is going to demonstrate that stemming happens automatically by elastic. Lunar, because. It's going to give us the same results Apple and Peach gave us. The other ones. Will be different because we're querying just. 'Apple different just peach, just banana. And that's going to change the Tf-Idf values of. The query. Document. But these three are going to have the exact same query document. So they should get the exact same outputs. For our search results. So for each query we are. Going to execute the. Query index dot search. We have our index that we created. we can search that query. Again there are options. Here that you can use to. configure various. Things. look through the elastic lunar documentation if you want to set that up or just do some of it manually on your own. So if we run this, we can. See. What sorts of results. We actually get back. So here are our search scores. And like I said, we can. Sort from lowest to highest. In fact they're already sorted from lowest to highest. So Elastic Lunar does that step for us as well. So when we search for just apples, the number one page. We got back was. The one with ID one, which is Apple. Party. It's all about apples. That makes sense that it has the highest score. Second. Highest score is the one. That's apple peaches. Also makes sense because it is a lot about apples, but it's less about apples than the ID one document. Document two doesn't show up. That's something you'll have to handle in your assignment, because the requirement for the assignment says if the. User or the client. Request ten documents, you have to send ten documents, even if there's no matches. But you can just add any documents if. Things are tied for zero. Anything is a valid. Score, and that is perfectly. Fine. So for apple, peach, peach, apple and Apple's peaches, we see exactly the same outputs, which we thought made sense. Since Apple and peach make up. Half the words we. See document one, which has ID zero, has the highest score. And that makes sense because it is a mix of apples and peaches, approximately 5050. Just like the query document, the other two both mentioned apples and both mentioned peaches. Or sorry. I guess document two mentions just apples. Document three mentions just peaches. They have search keywords that are above zero. They're just lower. Than what we would expect from the Apple Peach party document, which makes complete sense. If we query for peach. We get banana. Peach party is the first one. And Apple Peach Party is the second one. We don't get document two because it doesn't match. Peach doesn't mention peaches. Very similar. Scores. But that makes sense because they. Are a similar proportion of words are. Peach in this particular document. And obviously all three documents have the same ID values for peach. So that's not really going to change document to document. And when we query for banana we. Only get document three because it's the only one that mentions banana. So that is a very simple example of how you can use elastic loader in the most basic way. For the lab, you're obviously going to need. To incorporate more details. Into that. For the assignment. You're going to have to incorporate even more details. There is documentation though, so look. Through the. Elastic lunar documentation to see all the different things you can do as far as including Boolean models if you want to boosting specific fields. Customizing all of those indexing steps to tokenizing and the stemming and all of those things. You probably don't need to for the assignment. It's more if you want to or if you find it useful, especially for. Your. The pages that you're choosing for the assignment to. To crawl. That is not the fruits page. Customizing some of those steps might be helpful. But again, it's all up to you. That is how you can use Elastic Lunar, though, to build an index. Run your search. It's on. You to figure out how to make that interact with. Your database, how to. Incorporate PageRank, which. We'll talk about next year or not. Next year, next week. And get it all working for the assignment. But that's it for this week. We now know how we. Can do search. 'Typically using the vector space model Tf-Idf values. And we talked a little bit about scalability. Next week we will talk about PageRank. And that'll be. The last thing that you'll need to know. For the first. Assignment. And we'll kind of wrap up. The first. Unit of the course in. A way. The first half. I would say.